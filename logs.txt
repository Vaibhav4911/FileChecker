* 
* ==> Audit <==
* |---------|---------------|----------|----------------------|---------|---------------------|---------------------|
| Command |     Args      | Profile  |         User         | Version |     Start Time      |      End Time       |
|---------|---------------|----------|----------------------|---------|---------------------|---------------------|
| start   |               | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 05:50 IST | 28 Aug 23 06:02 IST |
| service | rapid-service | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 06:05 IST |                     |
| service | rapid-service | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 06:05 IST |                     |
| service | rapid-service | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 06:55 IST |                     |
| start   |               | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 10:49 IST | 28 Aug 23 10:49 IST |
| service | rapid-service | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 10:51 IST |                     |
| stop    |               | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 10:53 IST | 28 Aug 23 10:53 IST |
| start   |               | minikube | LAPTOP-T642IKCA\USER | v1.31.2 | 28 Aug 23 10:53 IST | 28 Aug 23 10:54 IST |
|---------|---------------|----------|----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/28 10:53:40
Running on machine: LAPTOP-T642IKCA
Binary: Built with gc go1.20.7 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0828 10:53:40.874184   28556 out.go:296] Setting OutFile to fd 108 ...
I0828 10:53:40.874184   28556 out.go:348] isatty.IsTerminal(108) = true
I0828 10:53:40.874184   28556 out.go:309] Setting ErrFile to fd 112...
I0828 10:53:40.874184   28556 out.go:348] isatty.IsTerminal(112) = true
W0828 10:53:40.889707   28556 root.go:314] Error reading config file at C:\Users\USER\.minikube\config\config.json: open C:\Users\USER\.minikube\config\config.json: The system cannot find the file specified.
I0828 10:53:40.897983   28556 out.go:303] Setting JSON to false
I0828 10:53:40.907484   28556 start.go:128] hostinfo: {"hostname":"LAPTOP-T642IKCA","uptime":109715,"bootTime":1693090505,"procs":317,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.2134 Build 22621.2134","kernelVersion":"10.0.22621.2134 Build 22621.2134","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"dffc36d4-dc5c-4d56-9f03-2d21cc6def2f"}
W0828 10:53:40.907484   28556 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0828 10:53:40.908694   28556 out.go:177] 😄  minikube v1.31.2 on Microsoft Windows 11 Home Single Language 10.0.22621.2134 Build 22621.2134
I0828 10:53:40.909746   28556 notify.go:220] Checking for updates...
I0828 10:53:40.909746   28556 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0828 10:53:40.909746   28556 driver.go:373] Setting default libvirt URI to qemu:///system
I0828 10:53:41.156426   28556 docker.go:121] docker version: linux-24.0.5:Docker Desktop 4.22.1 (118664)
I0828 10:53:41.165702   28556 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0828 10:53:41.647661   28556 info.go:266] docker info: {ID:8b20a0d5-76eb-4cad-b81f-d7f91e8f92cd Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:77 OomKillDisable:true NGoroutines:108 SystemTime:2023-08-28 05:23:41.603654511 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:24 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4014469120 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0828 10:53:41.649870   28556 out.go:177] ✨  Using the docker driver based on existing profile
I0828 10:53:41.650383   28556 start.go:298] selected driver: docker
I0828 10:53:41.650383   28556 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0828 10:53:41.650383   28556 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0828 10:53:41.673698   28556 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0828 10:53:42.049266   28556 info.go:266] docker info: {ID:8b20a0d5-76eb-4cad-b81f-d7f91e8f92cd Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:77 OomKillDisable:true NGoroutines:108 SystemTime:2023-08-28 05:23:41.988828671 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:24 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4014469120 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0828 10:53:42.167013   28556 cni.go:84] Creating CNI manager for ""
I0828 10:53:42.167013   28556 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0828 10:53:42.167013   28556 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0828 10:53:42.168051   28556 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0828 10:53:42.168575   28556 cache.go:122] Beginning downloading kic base image for docker with docker
I0828 10:53:42.169103   28556 out.go:177] 🚜  Pulling base image ...
I0828 10:53:42.169625   28556 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0828 10:53:42.169625   28556 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0828 10:53:42.170153   28556 preload.go:148] Found local preload: C:\Users\USER\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I0828 10:53:42.170153   28556 cache.go:57] Caching tarball of preloaded images
I0828 10:53:42.170153   28556 preload.go:174] Found C:\Users\USER\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0828 10:53:42.170153   28556 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0828 10:53:42.170668   28556 profile.go:148] Saving config to C:\Users\USER\.minikube\profiles\minikube\config.json ...
I0828 10:53:42.340862   28556 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0828 10:53:42.340862   28556 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0828 10:53:42.340862   28556 cache.go:195] Successfully downloaded all kic artifacts
I0828 10:53:42.340862   28556 start.go:365] acquiring machines lock for minikube: {Name:mkf9b2c0e3eda3858844b01c9a7f8988b59f66a7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0828 10:53:42.341384   28556 start.go:369] acquired machines lock for "minikube" in 521.7µs
I0828 10:53:42.341384   28556 start.go:96] Skipping create...Using existing machine configuration
I0828 10:53:42.341384   28556 fix.go:54] fixHost starting: 
I0828 10:53:42.358776   28556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0828 10:53:42.510321   28556 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0828 10:53:42.510428   28556 fix.go:128] unexpected machine state, will restart: <nil>
I0828 10:53:42.511530   28556 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0828 10:53:42.521036   28556 cli_runner.go:164] Run: docker start minikube
I0828 10:53:45.383358   28556 cli_runner.go:217] Completed: docker start minikube: (2.8623221s)
I0828 10:53:45.396588   28556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0828 10:53:45.567288   28556 kic.go:426] container "minikube" state is running.
I0828 10:53:45.581189   28556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0828 10:53:45.751637   28556 profile.go:148] Saving config to C:\Users\USER\.minikube\profiles\minikube\config.json ...
I0828 10:53:45.755018   28556 machine.go:88] provisioning docker machine ...
I0828 10:53:45.755018   28556 ubuntu.go:169] provisioning hostname "minikube"
I0828 10:53:45.763389   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:45.942583   28556 main.go:141] libmachine: Using SSH client type: native
I0828 10:53:45.943105   28556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb79400] 0xb7c2a0 <nil>  [] 0s} 127.0.0.1 64131 <nil> <nil>}
I0828 10:53:45.943105   28556 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0828 10:53:45.948226   28556 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0828 10:53:49.118870   28556 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0828 10:53:49.130641   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:49.359136   28556 main.go:141] libmachine: Using SSH client type: native
I0828 10:53:49.359655   28556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb79400] 0xb7c2a0 <nil>  [] 0s} 127.0.0.1 64131 <nil> <nil>}
I0828 10:53:49.359655   28556 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0828 10:53:49.505242   28556 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0828 10:53:49.505242   28556 ubuntu.go:175] set auth options {CertDir:C:\Users\USER\.minikube CaCertPath:C:\Users\USER\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\USER\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\USER\.minikube\machines\server.pem ServerKeyPath:C:\Users\USER\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\USER\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\USER\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\USER\.minikube}
I0828 10:53:49.505242   28556 ubuntu.go:177] setting up certificates
I0828 10:53:49.505242   28556 provision.go:83] configureAuth start
I0828 10:53:49.515422   28556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0828 10:53:49.690553   28556 provision.go:138] copyHostCerts
I0828 10:53:49.690553   28556 exec_runner.go:144] found C:\Users\USER\.minikube/key.pem, removing ...
I0828 10:53:49.690553   28556 exec_runner.go:203] rm: C:\Users\USER\.minikube\key.pem
I0828 10:53:49.691059   28556 exec_runner.go:151] cp: C:\Users\USER\.minikube\certs\key.pem --> C:\Users\USER\.minikube/key.pem (1679 bytes)
I0828 10:53:49.693229   28556 exec_runner.go:144] found C:\Users\USER\.minikube/ca.pem, removing ...
I0828 10:53:49.693229   28556 exec_runner.go:203] rm: C:\Users\USER\.minikube\ca.pem
I0828 10:53:49.693229   28556 exec_runner.go:151] cp: C:\Users\USER\.minikube\certs\ca.pem --> C:\Users\USER\.minikube/ca.pem (1074 bytes)
I0828 10:53:49.694897   28556 exec_runner.go:144] found C:\Users\USER\.minikube/cert.pem, removing ...
I0828 10:53:49.694897   28556 exec_runner.go:203] rm: C:\Users\USER\.minikube\cert.pem
I0828 10:53:49.694897   28556 exec_runner.go:151] cp: C:\Users\USER\.minikube\certs\cert.pem --> C:\Users\USER\.minikube/cert.pem (1115 bytes)
I0828 10:53:49.699720   28556 provision.go:112] generating server cert: C:\Users\USER\.minikube\machines\server.pem ca-key=C:\Users\USER\.minikube\certs\ca.pem private-key=C:\Users\USER\.minikube\certs\ca-key.pem org=USER.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0828 10:53:49.788796   28556 provision.go:172] copyRemoteCerts
I0828 10:53:49.806538   28556 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0828 10:53:49.816156   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:49.997748   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:53:50.049779   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0828 10:53:50.075444   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0828 10:53:50.100271   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0828 10:53:50.126016   28556 provision.go:86] duration metric: configureAuth took 619.7133ms
I0828 10:53:50.126016   28556 ubuntu.go:193] setting minikube options for container-runtime
I0828 10:53:50.126016   28556 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0828 10:53:50.135500   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:50.308816   28556 main.go:141] libmachine: Using SSH client type: native
I0828 10:53:50.309318   28556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb79400] 0xb7c2a0 <nil>  [] 0s} 127.0.0.1 64131 <nil> <nil>}
I0828 10:53:50.309318   28556 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0828 10:53:50.394654   28556 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0828 10:53:50.394654   28556 ubuntu.go:71] root file system type: overlay
I0828 10:53:50.394654   28556 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0828 10:53:50.403704   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:50.554670   28556 main.go:141] libmachine: Using SSH client type: native
I0828 10:53:50.555216   28556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb79400] 0xb7c2a0 <nil>  [] 0s} 127.0.0.1 64131 <nil> <nil>}
I0828 10:53:50.555216   28556 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0828 10:53:50.704680   28556 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0828 10:53:50.713687   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:50.874811   28556 main.go:141] libmachine: Using SSH client type: native
I0828 10:53:50.875401   28556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb79400] 0xb7c2a0 <nil>  [] 0s} 127.0.0.1 64131 <nil> <nil>}
I0828 10:53:50.875401   28556 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0828 10:53:51.029670   28556 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0828 10:53:51.029670   28556 machine.go:91] provisioned docker machine in 5.2746519s
I0828 10:53:51.029670   28556 start.go:300] post-start starting for "minikube" (driver="docker")
I0828 10:53:51.029670   28556 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0828 10:53:51.045183   28556 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0828 10:53:51.054197   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:51.209382   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:53:51.329763   28556 ssh_runner.go:195] Run: cat /etc/os-release
I0828 10:53:51.335298   28556 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0828 10:53:51.335298   28556 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0828 10:53:51.335298   28556 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0828 10:53:51.335298   28556 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0828 10:53:51.335826   28556 filesync.go:126] Scanning C:\Users\USER\.minikube\addons for local assets ...
I0828 10:53:51.335826   28556 filesync.go:126] Scanning C:\Users\USER\.minikube\files for local assets ...
I0828 10:53:51.336353   28556 start.go:303] post-start completed in 306.6829ms
I0828 10:53:51.351655   28556 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0828 10:53:51.360485   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:51.517555   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:53:51.643699   28556 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0828 10:53:51.649285   28556 fix.go:56] fixHost completed within 9.3079005s
I0828 10:53:51.649285   28556 start.go:83] releasing machines lock for "minikube", held for 9.3079005s
I0828 10:53:51.658339   28556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0828 10:53:51.828593   28556 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0828 10:53:51.839654   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:51.845029   28556 ssh_runner.go:195] Run: cat /version.json
I0828 10:53:51.854016   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:53:52.009611   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:53:52.024954   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:53:52.500613   28556 ssh_runner.go:195] Run: systemctl --version
I0828 10:53:52.523163   28556 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0828 10:53:52.556477   28556 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0828 10:53:52.567483   28556 start.go:410] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0828 10:53:52.582443   28556 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0828 10:53:52.592960   28556 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0828 10:53:52.592960   28556 start.go:466] detecting cgroup driver to use...
I0828 10:53:52.592960   28556 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0828 10:53:52.592960   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0828 10:53:52.624592   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0828 10:53:52.650303   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0828 10:53:52.662268   28556 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0828 10:53:52.677186   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0828 10:53:52.705110   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0828 10:53:52.731400   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0828 10:53:52.757228   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0828 10:53:52.782950   28556 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0828 10:53:52.808487   28556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0828 10:53:52.834955   28556 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0828 10:53:52.861078   28556 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0828 10:53:52.885322   28556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0828 10:53:52.987959   28556 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0828 10:53:53.099543   28556 start.go:466] detecting cgroup driver to use...
I0828 10:53:53.099543   28556 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0828 10:53:53.121075   28556 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0828 10:53:53.134712   28556 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0828 10:53:53.152625   28556 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0828 10:53:53.166837   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0828 10:53:53.231906   28556 ssh_runner.go:195] Run: which cri-dockerd
I0828 10:53:53.262904   28556 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0828 10:53:53.273719   28556 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0828 10:53:53.313025   28556 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0828 10:53:53.418273   28556 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0828 10:53:53.500968   28556 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0828 10:53:53.500968   28556 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0828 10:53:53.534389   28556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0828 10:53:53.647877   28556 ssh_runner.go:195] Run: sudo systemctl restart docker
I0828 10:53:58.156331   28556 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.5084537s)
I0828 10:53:58.172802   28556 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0828 10:53:58.285033   28556 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0828 10:53:58.402465   28556 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0828 10:53:58.520198   28556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0828 10:53:58.642192   28556 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0828 10:53:58.682184   28556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0828 10:53:58.790427   28556 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0828 10:53:58.949188   28556 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0828 10:53:58.968100   28556 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0828 10:53:58.973004   28556 start.go:534] Will wait 60s for crictl version
I0828 10:53:58.996203   28556 ssh_runner.go:195] Run: which crictl
I0828 10:53:59.023278   28556 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0828 10:53:59.146766   28556 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0828 10:53:59.156724   28556 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0828 10:53:59.197549   28556 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0828 10:53:59.221073   28556 out.go:204] 🐳  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I0828 10:53:59.233677   28556 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0828 10:53:59.592990   28556 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0828 10:53:59.619531   28556 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0828 10:53:59.625240   28556 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0828 10:53:59.648809   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0828 10:53:59.808633   28556 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0828 10:53:59.818607   28556 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0828 10:53:59.837684   28556 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0828 10:53:59.837684   28556 docker.go:566] Images already preloaded, skipping extraction
I0828 10:53:59.847692   28556 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0828 10:53:59.866925   28556 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0828 10:53:59.867440   28556 cache_images.go:84] Images are preloaded, skipping loading
I0828 10:53:59.879193   28556 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0828 10:54:00.005023   28556 cni.go:84] Creating CNI manager for ""
I0828 10:54:00.005023   28556 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0828 10:54:00.006589   28556 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0828 10:54:00.006589   28556 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0828 10:54:00.006589   28556 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0828 10:54:00.006589   28556 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0828 10:54:00.031202   28556 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0828 10:54:00.042305   28556 binaries.go:44] Found k8s binaries, skipping transfer
I0828 10:54:00.067681   28556 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0828 10:54:00.078000   28556 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0828 10:54:00.094982   28556 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0828 10:54:00.112501   28556 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0828 10:54:00.153072   28556 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0828 10:54:00.158359   28556 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0828 10:54:00.172682   28556 certs.go:56] Setting up C:\Users\USER\.minikube\profiles\minikube for IP: 192.168.49.2
I0828 10:54:00.172682   28556 certs.go:190] acquiring lock for shared ca certs: {Name:mkc83777d01c7a642f9f4b7b50df4b1d932d95c7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0828 10:54:00.180561   28556 certs.go:199] skipping minikubeCA CA generation: C:\Users\USER\.minikube\ca.key
I0828 10:54:00.187321   28556 certs.go:199] skipping proxyClientCA CA generation: C:\Users\USER\.minikube\proxy-client-ca.key
I0828 10:54:00.188463   28556 certs.go:315] skipping minikube-user signed cert generation: C:\Users\USER\.minikube\profiles\minikube\client.key
I0828 10:54:00.191153   28556 certs.go:315] skipping minikube signed cert generation: C:\Users\USER\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0828 10:54:00.192793   28556 certs.go:315] skipping aggregator signed cert generation: C:\Users\USER\.minikube\profiles\minikube\proxy-client.key
I0828 10:54:00.193829   28556 certs.go:437] found cert: C:\Users\USER\.minikube\certs\C:\Users\USER\.minikube\certs\ca-key.pem (1679 bytes)
I0828 10:54:00.194350   28556 certs.go:437] found cert: C:\Users\USER\.minikube\certs\C:\Users\USER\.minikube\certs\ca.pem (1074 bytes)
I0828 10:54:00.195908   28556 certs.go:437] found cert: C:\Users\USER\.minikube\certs\C:\Users\USER\.minikube\certs\cert.pem (1115 bytes)
I0828 10:54:00.195908   28556 certs.go:437] found cert: C:\Users\USER\.minikube\certs\C:\Users\USER\.minikube\certs\key.pem (1679 bytes)
I0828 10:54:00.196951   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0828 10:54:00.221830   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0828 10:54:00.247780   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0828 10:54:00.271898   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0828 10:54:00.295370   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0828 10:54:00.321847   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0828 10:54:00.345932   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0828 10:54:00.369818   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0828 10:54:00.393836   28556 ssh_runner.go:362] scp C:\Users\USER\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0828 10:54:00.416895   28556 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0828 10:54:00.463640   28556 ssh_runner.go:195] Run: openssl version
I0828 10:54:00.493904   28556 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0828 10:54:00.527749   28556 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0828 10:54:00.534471   28556 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug 28 00:32 /usr/share/ca-certificates/minikubeCA.pem
I0828 10:54:00.552460   28556 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0828 10:54:00.576303   28556 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0828 10:54:00.603794   28556 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0828 10:54:00.625372   28556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0828 10:54:00.655038   28556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0828 10:54:00.689374   28556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0828 10:54:00.715598   28556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0828 10:54:00.744053   28556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0828 10:54:00.786918   28556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0828 10:54:00.798024   28556 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0828 10:54:00.818139   28556 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0828 10:54:00.860838   28556 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0828 10:54:00.872675   28556 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0828 10:54:00.872675   28556 kubeadm.go:636] restartCluster start
I0828 10:54:00.904035   28556 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0828 10:54:00.914745   28556 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0828 10:54:00.931529   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0828 10:54:01.113498   28556 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\USER\.kube\config
I0828 10:54:01.114009   28556 kubeconfig.go:146] "minikube" context is missing from C:\Users\USER\.kube\config - will repair!
I0828 10:54:01.114009   28556 lock.go:35] WriteFile acquiring C:\Users\USER\.kube\config: {Name:mk337b1533ed7fcb58cddea65bf7e3ba0a1c487e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0828 10:54:01.154097   28556 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0828 10:54:01.164441   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:01.186366   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:01.197937   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:01.197937   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:01.219110   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:01.229778   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:01.731753   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:01.750502   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:01.761390   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:02.244119   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:02.273521   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:02.286661   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:02.742203   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:02.760672   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:02.772317   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:03.237773   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:03.254192   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:03.265567   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:03.744801   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:03.759333   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:03.770886   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:04.241619   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:04.257641   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:04.269610   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:04.741583   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:04.756340   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:04.767922   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:05.244291   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:05.259811   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:05.273264   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:05.740629   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:05.759984   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:05.772911   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:06.242484   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:06.260224   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:06.271051   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:06.740595   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:06.758314   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:06.770586   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:07.240417   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:07.256046   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:07.273997   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:07.743751   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:07.761738   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:07.772689   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:08.229809   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:08.246583   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:08.258476   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:08.742144   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:08.756552   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:08.768367   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:09.241649   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:09.257100   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:09.269017   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:09.744374   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:09.758834   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:09.771750   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:10.229859   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:10.247885   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:10.259870   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:10.741553   28556 api_server.go:166] Checking apiserver status ...
I0828 10:54:10.757281   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0828 10:54:10.769942   28556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0828 10:54:11.181134   28556 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0828 10:54:11.181740   28556 kubeadm.go:1128] stopping kube-system containers ...
I0828 10:54:11.190774   28556 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0828 10:54:11.236197   28556 docker.go:462] Stopping containers: [a0a0a3d955d3 89b50d64b137 6d2e08c99562 a925d3a1dd48 707a597f2ede 42fefb462bc7 8bab43343013 1cc7d2eb70cf 6af384330d88 d0def8ef7e38 6655355e192c 26576f4bd215 311c9f4d02ce 1d378ebb02b4 9a25368847c2 4fa12ca2f885 94ad68007826 70dbb2f30afa 96a45cf9ce7d c999fd81c548 bef87ed99b47 0c76a9ceb166 b34b37a8389a bebe6f7438cc 2f9325865387 6724565515be 922248e2dcdb]
I0828 10:54:11.244691   28556 ssh_runner.go:195] Run: docker stop a0a0a3d955d3 89b50d64b137 6d2e08c99562 a925d3a1dd48 707a597f2ede 42fefb462bc7 8bab43343013 1cc7d2eb70cf 6af384330d88 d0def8ef7e38 6655355e192c 26576f4bd215 311c9f4d02ce 1d378ebb02b4 9a25368847c2 4fa12ca2f885 94ad68007826 70dbb2f30afa 96a45cf9ce7d c999fd81c548 bef87ed99b47 0c76a9ceb166 b34b37a8389a bebe6f7438cc 2f9325865387 6724565515be 922248e2dcdb
I0828 10:54:11.293142   28556 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0828 10:54:11.321163   28556 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0828 10:54:11.330816   28556 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Aug 28 00:32 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Aug 28 00:32 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Aug 28 00:32 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Aug 28 00:32 /etc/kubernetes/scheduler.conf

I0828 10:54:11.347677   28556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0828 10:54:11.375425   28556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0828 10:54:11.400922   28556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0828 10:54:11.411106   28556 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0828 10:54:11.426032   28556 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0828 10:54:11.450966   28556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0828 10:54:11.461805   28556 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0828 10:54:11.478802   28556 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0828 10:54:11.503560   28556 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0828 10:54:11.514022   28556 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0828 10:54:11.514022   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0828 10:54:11.925080   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0828 10:54:13.054276   28556 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.1291956s)
I0828 10:54:13.054276   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0828 10:54:13.258836   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0828 10:54:13.361400   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0828 10:54:13.457445   28556 api_server.go:52] waiting for apiserver process to appear ...
I0828 10:54:13.477708   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:13.507914   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:14.041909   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:14.543049   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:15.039201   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:15.546537   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:16.046861   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:16.117085   28556 api_server.go:72] duration metric: took 2.65964s to wait for apiserver process to appear ...
I0828 10:54:16.117085   28556 api_server.go:88] waiting for apiserver healthz status ...
I0828 10:54:16.117712   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:16.124168   28556 api_server.go:269] stopped: https://127.0.0.1:64135/healthz: Get "https://127.0.0.1:64135/healthz": EOF
I0828 10:54:16.124168   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:16.127336   28556 api_server.go:269] stopped: https://127.0.0.1:64135/healthz: Get "https://127.0.0.1:64135/healthz": EOF
I0828 10:54:16.633768   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:19.019830   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0828 10:54:19.020344   28556 api_server.go:103] status: https://127.0.0.1:64135/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0828 10:54:19.020344   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:19.128326   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0828 10:54:19.128326   28556 api_server.go:103] status: https://127.0.0.1:64135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0828 10:54:19.128326   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:19.217443   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0828 10:54:19.217443   28556 api_server.go:103] status: https://127.0.0.1:64135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0828 10:54:19.639522   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:19.646958   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0828 10:54:19.646958   28556 api_server.go:103] status: https://127.0.0.1:64135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0828 10:54:20.141519   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:20.164775   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0828 10:54:20.164775   28556 api_server.go:103] status: https://127.0.0.1:64135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0828 10:54:20.639543   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:20.648025   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 200:
ok
I0828 10:54:20.670363   28556 api_server.go:141] control plane version: v1.27.4
I0828 10:54:20.670363   28556 api_server.go:131] duration metric: took 4.5532777s to wait for apiserver health ...
I0828 10:54:20.670363   28556 cni.go:84] Creating CNI manager for ""
I0828 10:54:20.670363   28556 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0828 10:54:20.671937   28556 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0828 10:54:20.698796   28556 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0828 10:54:20.719874   28556 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0828 10:54:20.742769   28556 system_pods.go:43] waiting for kube-system pods to appear ...
I0828 10:54:20.766290   28556 system_pods.go:59] 7 kube-system pods found
I0828 10:54:20.766290   28556 system_pods.go:61] "coredns-5d78c9869d-kk4r4" [021759f5-17bb-4e9a-b3eb-c04f9f80c190] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0828 10:54:20.766290   28556 system_pods.go:61] "etcd-minikube" [1b7b0a2a-5980-4deb-a74b-71eb2d11ad35] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0828 10:54:20.766290   28556 system_pods.go:61] "kube-apiserver-minikube" [13a6ff9d-782e-4ea4-b576-7a01967502af] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0828 10:54:20.766290   28556 system_pods.go:61] "kube-controller-manager-minikube" [f39ba4f2-1e21-4966-9d3f-c6e9544bb9ed] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0828 10:54:20.766290   28556 system_pods.go:61] "kube-proxy-94bch" [639d3a42-2d70-4b32-ae79-e60fecefeb46] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0828 10:54:20.766290   28556 system_pods.go:61] "kube-scheduler-minikube" [827b4634-4e5e-4715-956c-c0d2ef4971ed] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0828 10:54:20.766290   28556 system_pods.go:61] "storage-provisioner" [c0d5b174-8c19-4600-8bbd-b330374c9478] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0828 10:54:20.766290   28556 system_pods.go:74] duration metric: took 23.5208ms to wait for pod list to return data ...
I0828 10:54:20.766290   28556 node_conditions.go:102] verifying NodePressure condition ...
I0828 10:54:20.772106   28556 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0828 10:54:20.772106   28556 node_conditions.go:123] node cpu capacity is 8
I0828 10:54:20.772644   28556 node_conditions.go:105] duration metric: took 6.3534ms to run NodePressure ...
I0828 10:54:20.772644   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0828 10:54:22.020483   28556 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.2478392s)
I0828 10:54:22.020483   28556 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0828 10:54:22.032427   28556 ops.go:34] apiserver oom_adj: -16
I0828 10:54:22.032427   28556 kubeadm.go:640] restartCluster took 21.1597524s
I0828 10:54:22.032427   28556 kubeadm.go:406] StartCluster complete in 21.2344028s
I0828 10:54:22.032427   28556 settings.go:142] acquiring lock: {Name:mkcd282b810ea7bce3652a3109f04f5e9f9f01ff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0828 10:54:22.032955   28556 settings.go:150] Updating kubeconfig:  C:\Users\USER\.kube\config
I0828 10:54:22.035091   28556 lock.go:35] WriteFile acquiring C:\Users\USER\.kube\config: {Name:mk337b1533ed7fcb58cddea65bf7e3ba0a1c487e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0828 10:54:22.037280   28556 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0828 10:54:22.037802   28556 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0828 10:54:22.037280   28556 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0828 10:54:22.040634   28556 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0828 10:54:22.040634   28556 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0828 10:54:22.040634   28556 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0828 10:54:22.040634   28556 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0828 10:54:22.040634   28556 addons.go:240] addon storage-provisioner should already be in state true
I0828 10:54:22.043335   28556 host.go:66] Checking if "minikube" exists ...
I0828 10:54:22.094114   28556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0828 10:54:22.094114   28556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0828 10:54:22.109968   28556 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0828 10:54:22.111004   28556 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0828 10:54:22.111526   28556 out.go:177] 🔎  Verifying Kubernetes components...
I0828 10:54:22.143946   28556 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0828 10:54:22.384557   28556 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0828 10:54:22.387787   28556 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0828 10:54:22.387787   28556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0828 10:54:22.396696   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:54:22.420562   28556 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0828 10:54:22.420562   28556 addons.go:240] addon default-storageclass should already be in state true
I0828 10:54:22.420562   28556 host.go:66] Checking if "minikube" exists ...
I0828 10:54:22.453371   28556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0828 10:54:22.620366   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:54:22.636257   28556 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0828 10:54:22.636257   28556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0828 10:54:22.648262   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0828 10:54:22.826007   28556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64131 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0828 10:54:22.851047   28556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0828 10:54:22.917775   28556 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0828 10:54:22.931050   28556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0828 10:54:23.118040   28556 api_server.go:52] waiting for apiserver process to appear ...
I0828 10:54:23.136846   28556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0828 10:54:23.246934   28556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0828 10:54:23.855080   28556 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0040325s)
I0828 10:54:23.855080   28556 api_server.go:72] duration metric: took 1.7440753s to wait for apiserver process to appear ...
I0828 10:54:23.855080   28556 api_server.go:88] waiting for apiserver healthz status ...
I0828 10:54:23.855080   28556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64135/healthz ...
I0828 10:54:23.856136   28556 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0828 10:54:23.857182   28556 addons.go:502] enable addons completed in 1.8204067s: enabled=[storage-provisioner default-storageclass]
I0828 10:54:23.863542   28556 api_server.go:279] https://127.0.0.1:64135/healthz returned 200:
ok
I0828 10:54:23.865094   28556 api_server.go:141] control plane version: v1.27.4
I0828 10:54:23.865094   28556 api_server.go:131] duration metric: took 10.0138ms to wait for apiserver health ...
I0828 10:54:23.865094   28556 system_pods.go:43] waiting for kube-system pods to appear ...
I0828 10:54:23.875263   28556 system_pods.go:59] 7 kube-system pods found
I0828 10:54:23.875263   28556 system_pods.go:61] "coredns-5d78c9869d-kk4r4" [021759f5-17bb-4e9a-b3eb-c04f9f80c190] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0828 10:54:23.875263   28556 system_pods.go:61] "etcd-minikube" [1b7b0a2a-5980-4deb-a74b-71eb2d11ad35] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0828 10:54:23.875263   28556 system_pods.go:61] "kube-apiserver-minikube" [13a6ff9d-782e-4ea4-b576-7a01967502af] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0828 10:54:23.875263   28556 system_pods.go:61] "kube-controller-manager-minikube" [f39ba4f2-1e21-4966-9d3f-c6e9544bb9ed] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0828 10:54:23.875263   28556 system_pods.go:61] "kube-proxy-94bch" [639d3a42-2d70-4b32-ae79-e60fecefeb46] Running
I0828 10:54:23.875263   28556 system_pods.go:61] "kube-scheduler-minikube" [827b4634-4e5e-4715-956c-c0d2ef4971ed] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0828 10:54:23.875263   28556 system_pods.go:61] "storage-provisioner" [c0d5b174-8c19-4600-8bbd-b330374c9478] Running
I0828 10:54:23.875263   28556 system_pods.go:74] duration metric: took 10.1695ms to wait for pod list to return data ...
I0828 10:54:23.875263   28556 kubeadm.go:581] duration metric: took 1.7642586s to wait for : map[apiserver:true system_pods:true] ...
I0828 10:54:23.875263   28556 node_conditions.go:102] verifying NodePressure condition ...
I0828 10:54:23.883827   28556 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0828 10:54:23.883827   28556 node_conditions.go:123] node cpu capacity is 8
I0828 10:54:23.883827   28556 node_conditions.go:105] duration metric: took 8.564ms to run NodePressure ...
I0828 10:54:23.883827   28556 start.go:228] waiting for startup goroutines ...
I0828 10:54:23.883827   28556 start.go:233] waiting for cluster config update ...
I0828 10:54:23.883827   28556 start.go:242] writing updated cluster config ...
I0828 10:54:23.908502   28556 ssh_runner.go:195] Run: rm -f paused
I0828 10:54:24.073265   28556 start.go:600] kubectl: 1.27.2, cluster: 1.27.4 (minor skew: 0)
I0828 10:54:24.073770   28556 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.797246941Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.797249998Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.797268472Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.797321946Z" level=info msg="Daemon has completed initialization"
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.838855437Z" level=info msg="API listen on /var/run/docker.sock"
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.838877912Z" level=info msg="API listen on [::]:2376"
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.840070916Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.840606382Z" level=info msg="Daemon shutdown complete"
Aug 28 05:23:55 minikube dockerd[714]: time="2023-08-28T05:23:55.840719742Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Aug 28 05:23:55 minikube systemd[1]: docker.service: Deactivated successfully.
Aug 28 05:23:55 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 28 05:23:55 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 28 05:23:55 minikube dockerd[936]: time="2023-08-28T05:23:55.883534764Z" level=info msg="Starting up"
Aug 28 05:23:55 minikube dockerd[936]: time="2023-08-28T05:23:55.894339760Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 28 05:23:55 minikube dockerd[936]: time="2023-08-28T05:23:55.905247517Z" level=info msg="Loading containers: start."
Aug 28 05:23:57 minikube dockerd[936]: time="2023-08-28T05:23:57.414703339Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.099234552Z" level=info msg="Loading containers: done."
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.122389579Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.122435959Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.122441782Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.122445078Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.122463003Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.122513356Z" level=info msg="Daemon has completed initialization"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.153546442Z" level=info msg="API listen on /var/run/docker.sock"
Aug 28 05:23:58 minikube dockerd[936]: time="2023-08-28T05:23:58.153550770Z" level=info msg="API listen on [::]:2376"
Aug 28 05:23:58 minikube systemd[1]: Started Docker Application Container Engine.
Aug 28 05:23:58 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Start docker client with request timeout 0s"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Loaded network plugin cni"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Docker Info: &{ID:0914b76d-a5e4-4f5d-b699-8512dad1d46d Containers:28 ContainersRunning:0 ContainersPaused:0 ContainersStopped:28 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:35 SystemTime:2023-08-28T05:23:58.938929715Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0000f6620 NCPU:8 MemTotal:4014469120 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 28 05:23:58 minikube cri-dockerd[1180]: time="2023-08-28T05:23:58Z" level=info msg="Start cri-dockerd grpc backend"
Aug 28 05:23:58 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 28 05:24:14 minikube cri-dockerd[1180]: time="2023-08-28T05:24:14Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-kk4r4_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6af384330d889e145ea9f6c5f634c229c805f8bfa049f772c258697ef7be471b\""
Aug 28 05:24:14 minikube cri-dockerd[1180]: time="2023-08-28T05:24:14Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"rapid-app-5c4b978988-djzm9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d0e51fe7ee044f2d1395312d588de2f6ce2370bf11cf0911b36037d54c7e3057\""
Aug 28 05:24:15 minikube cri-dockerd[1180]: time="2023-08-28T05:24:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d7d427726d74d09226e4ac25d363cb81aa13c8b76e77524fe16dd2479922076b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:15 minikube cri-dockerd[1180]: time="2023-08-28T05:24:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ac7a06ae09d733d9b472344350676c9a95aaf97db2331b8eb333a609f603c25f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:15 minikube cri-dockerd[1180]: time="2023-08-28T05:24:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cd0c919491f325490603d033034756eae7ec78d101bce160ba8952c6fcd9e152/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:15 minikube cri-dockerd[1180]: time="2023-08-28T05:24:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/571e1779a9c1fe402135f79e2a0310958cd8ffec95c14e0100fbaf1617c3b72f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:19 minikube cri-dockerd[1180]: time="2023-08-28T05:24:19Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 28 05:24:21 minikube cri-dockerd[1180]: time="2023-08-28T05:24:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fff52da0be11ed4a93cba51d1abfcdb146fa9e94dccf5dd05b38463972e3f455/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:21 minikube cri-dockerd[1180]: time="2023-08-28T05:24:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/65bc31b6d77dbcb19a687e1e2b869162c86d4304cd91d2c7e821d9588aa1e7d5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:21 minikube cri-dockerd[1180]: time="2023-08-28T05:24:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/34dae6cbe915f19b2354e6643216597b9b649fe686e60e038f6862bd8bc58a31/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 28 05:24:21 minikube cri-dockerd[1180]: time="2023-08-28T05:24:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0650dd9818f4f1b945e2d7e9730bc7418d9e9a78a86eb653e6ba41e14671d150/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 28 05:24:26 minikube dockerd[936]: time="2023-08-28T05:24:26.268622230Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 28 05:24:26 minikube dockerd[936]: time="2023-08-28T05:24:26.268768089Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 28 05:24:32 minikube dockerd[936]: time="2023-08-28T05:24:32.053050041Z" level=info msg="ignoring event" container=1240730520b5d61f62e0d117e4bedf677fdeeeb03cae76758df8354234bfef28 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 28 05:24:44 minikube dockerd[936]: time="2023-08-28T05:24:44.507724924Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 28 05:24:44 minikube dockerd[936]: time="2023-08-28T05:24:44.507789485Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 28 05:25:11 minikube dockerd[936]: time="2023-08-28T05:25:11.264969298Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 28 05:25:11 minikube dockerd[936]: time="2023-08-28T05:25:11.265026862Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 28 05:26:03 minikube dockerd[936]: time="2023-08-28T05:26:03.371965525Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 28 05:26:03 minikube dockerd[936]: time="2023-08-28T05:26:03.372028290Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 28 05:27:37 minikube dockerd[936]: time="2023-08-28T05:27:37.367181543Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 28 05:27:37 minikube dockerd[936]: time="2023-08-28T05:27:37.367238468Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
198eeb19dcdf9       6e38f40d628db       4 minutes ago       Running             storage-provisioner       5                   65bc31b6d77db       storage-provisioner
aa0c97f988593       ead0a4a53df89       5 minutes ago       Running             coredns                   2                   0650dd9818f4f       coredns-5d78c9869d-kk4r4
1240730520b5d       6e38f40d628db       5 minutes ago       Exited              storage-provisioner       4                   65bc31b6d77db       storage-provisioner
05389b7164371       6848d7eda0341       5 minutes ago       Running             kube-proxy                2                   fff52da0be11e       kube-proxy-94bch
8405c9baf1819       f466468864b7a       5 minutes ago       Running             kube-controller-manager   2                   cd0c919491f32       kube-controller-manager-minikube
5bde31daf45ae       e7972205b6614       5 minutes ago       Running             kube-apiserver            2                   d7d427726d74d       kube-apiserver-minikube
e06c585176942       98ef2570f3cde       5 minutes ago       Running             kube-scheduler            2                   571e1779a9c1f       kube-scheduler-minikube
03860f5b048a1       86b6af7dd652c       5 minutes ago       Running             etcd                      2                   ac7a06ae09d73       etcd-minikube
89b50d64b1374       ead0a4a53df89       10 minutes ago      Exited              coredns                   1                   6af384330d889       coredns-5d78c9869d-kk4r4
6d2e08c99562c       86b6af7dd652c       10 minutes ago      Exited              etcd                      1                   26576f4bd215e       etcd-minikube
707a597f2ede8       f466468864b7a       10 minutes ago      Exited              kube-controller-manager   1                   d0def8ef7e388       kube-controller-manager-minikube
42fefb462bc71       98ef2570f3cde       10 minutes ago      Exited              kube-scheduler            1                   6655355e192ca       kube-scheduler-minikube
8bab433430134       e7972205b6614       10 minutes ago      Exited              kube-apiserver            1                   1d378ebb02b40       kube-apiserver-minikube
1cc7d2eb70cf6       6848d7eda0341       10 minutes ago      Exited              kube-proxy                1                   311c9f4d02cef       kube-proxy-94bch

* 
* ==> coredns [89b50d64b137] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:40620 - 26724 "HINFO IN 2604273308727766561.985610370786807412. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.031726629s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [aa0c97f98859] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:34463 - 17640 "HINFO IN 1752646205908181545.1478815866468424631. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.029114794s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": net/http: TLS handshake timeout

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_28T06_02_35_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 28 Aug 2023 00:32:30 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 28 Aug 2023 05:29:34 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 28 Aug 2023 05:29:25 +0000   Mon, 28 Aug 2023 00:32:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 28 Aug 2023 05:29:25 +0000   Mon, 28 Aug 2023 00:32:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 28 Aug 2023 05:29:25 +0000   Mon, 28 Aug 2023 00:32:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 28 Aug 2023 05:29:25 +0000   Mon, 28 Aug 2023 00:32:30 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3920380Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3920380Ki
  pods:               110
System Info:
  Machine ID:                 09e03888ee1d46bd81b47573d4790190
  System UUID:                09e03888ee1d46bd81b47573d4790190
  Boot ID:                    070a1b9b-d05a-40b7-8721-dd1f2b1cad90
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     rapid-app-5c4b978988-djzm9          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h56m
  kube-system                 coredns-5d78c9869d-kk4r4            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     4h56m
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         4h57m
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h57m
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h57m
  kube-system                 kube-proxy-94bch                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h56m
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h57m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h57m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 9m59s                  kube-proxy       
  Normal  Starting                 5m15s                  kube-proxy       
  Normal  RegisteredNode           9m48s                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 5m23s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  5m23s (x8 over 5m23s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m23s (x8 over 5m23s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m23s (x7 over 5m23s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  5m23s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           5m6s                   node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Aug28 02:11] PCI: Fatal: No config space access function found
[  +0.072079] PCI: System does not support PCI
[  +0.235021] kvm: already loaded the other module
[  +0.657974] hv_utils: cannot register PTP clock: 0
[  +2.539980] FS-Cache: Duplicate cookie detected
[  +0.000541] FS-Cache: O-cookie c=0000000030f287c1 [p=00000000eb72e3ba fl=222 nc=0 na=1]
[  +0.000473] FS-Cache: O-cookie d=00000000309e09bb n=0000000021c2dae6
[  +0.000394] FS-Cache: O-key=[10] '34323934393337363531'
[  +0.000234] FS-Cache: N-cookie c=0000000071d3721b [p=00000000eb72e3ba fl=2 nc=0 na=1]
[  +0.000313] FS-Cache: N-cookie d=00000000309e09bb n=00000000141bbdcb
[  +0.000335] FS-Cache: N-key=[10] '34323934393337363531'
[  +0.464628] 9pnet_virtio: no channels available for device drvfs
[  +0.000515] WARNING: mount: waiting for virtio device...
[  +1.326781] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.026092] 9pnet_virtio: no channels available for device drvfs
[  +0.000782] WARNING: mount: waiting for virtio device...
[  +0.132234] WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.785418] FS-Cache: Duplicate cookie detected
[  +0.000609] FS-Cache: O-cookie c=00000000275970f9 [p=00000000eb72e3ba fl=222 nc=0 na=1]
[  +0.000632] FS-Cache: O-cookie d=00000000309e09bb n=00000000543510ef
[  +0.000504] FS-Cache: O-key=[10] '34323934393337393235'
[  +0.000399] FS-Cache: N-cookie c=00000000ffd5a417 [p=00000000eb72e3ba fl=2 nc=0 na=1]
[  +0.000597] FS-Cache: N-cookie d=00000000309e09bb n=0000000097435ef5
[  +0.000496] FS-Cache: N-key=[10] '34323934393337393235'
[  +0.000779] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.001716] init: (2) ERROR: UtilCreateProcessAndWait:702: /bin/mount failed with 2
[  +0.000702] init: (1) ERROR: UtilCreateProcessAndWait:722: /bin/mount failed with status 0x
[  +0.000002] ff00
[  +0.000769] init: (1) ERROR: ConfigMountFsTab:2484: Processing fstab with mount -a failed.
[  +0.016407] 9pnet_virtio: no channels available for device drvfs
[  +0.000465] WARNING: mount: waiting for virtio device...
[  +0.130411] WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.079548] init: (8) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.000793] init: (8) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2
[  +0.244395] init: (15) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.000711] init: (15) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2
[ +17.704153] cgroup: runc (760) created nested cgroup for controller "memory" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.
[  +0.001785] cgroup: "memory" requires setting use_hierarchy to 1 on the root
[Aug28 05:23] systemd-journald[223]: Failed to send WATCHDOG=1 notification message: Connection refused

* 
* ==> etcd [03860f5b048a] <==
* {"level":"info","ts":"2023-08-28T05:24:15.917Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-08-28T05:24:15.918Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-28T05:24:15.918Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-28T05:24:15.918Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-28T05:24:15.919Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-28T05:24:15.920Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-28T05:24:15.930Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"9.784811ms"}
{"level":"info","ts":"2023-08-28T05:24:16.032Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2023-08-28T05:24:16.115Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":6173}
{"level":"info","ts":"2023-08-28T05:24:16.115Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2023-08-28T05:24:16.116Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2023-08-28T05:24:16.116Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 6173, applied: 0, lastindex: 6173, lastterm: 3]"}
{"level":"warn","ts":"2023-08-28T05:24:16.120Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-28T05:24:16.122Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":4292}
{"level":"info","ts":"2023-08-28T05:24:16.130Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":5002}
{"level":"info","ts":"2023-08-28T05:24:16.132Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-28T05:24:16.135Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-08-28T05:24:16.135Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-28T05:24:16.136Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-08-28T05:24:16.136Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-28T05:24:16.137Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-28T05:24:16.137Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-28T05:24:16.137Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-08-28T05:24:16.138Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-28T05:24:16.139Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-28T05:24:16.139Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-28T05:24:16.138Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-28T05:24:16.143Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-28T05:24:16.143Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-28T05:24:16.138Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-28T05:24:16.143Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-28T05:24:16.137Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2023-08-28T05:24:17.818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-08-28T05:24:17.842Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-28T05:24:17.842Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-28T05:24:17.842Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-28T05:24:17.843Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-28T05:24:17.843Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-28T05:24:17.845Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-28T05:24:17.845Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}

* 
* ==> etcd [6d2e08c99562] <==
* {"level":"info","ts":"2023-08-28T05:19:32.720Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-08-28T05:19:32.721Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-28T05:19:32.721Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-28T05:19:32.729Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-28T05:19:32.810Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-28T05:19:32.812Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-28T05:19:32.821Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"6.203575ms"}
{"level":"info","ts":"2023-08-28T05:19:33.926Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2023-08-28T05:19:34.207Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":5862}
{"level":"info","ts":"2023-08-28T05:19:34.211Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2023-08-28T05:19:34.212Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2023-08-28T05:19:34.212Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 5862, applied: 0, lastindex: 5862, lastterm: 2]"}
{"level":"warn","ts":"2023-08-28T05:19:34.215Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-28T05:19:34.217Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":4292}
{"level":"info","ts":"2023-08-28T05:19:34.221Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":4742}
{"level":"info","ts":"2023-08-28T05:19:34.226Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-28T05:19:34.231Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-08-28T05:19:34.232Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-28T05:19:34.232Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-08-28T05:19:34.233Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2023-08-28T05:19:34.233Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-28T05:19:34.234Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-28T05:19:34.234Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-28T05:19:34.307Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-28T05:19:34.307Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-08-28T05:19:34.308Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-28T05:19:34.309Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-28T05:19:34.308Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-28T05:19:34.308Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-28T05:19:34.308Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-28T05:19:34.310Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-28T05:19:34.310Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2023-08-28T05:19:35.814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-08-28T05:19:35.821Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-28T05:19:35.821Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-28T05:19:35.821Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-28T05:19:35.821Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-28T05:19:35.821Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-28T05:19:35.826Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-08-28T05:19:35.826Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-28T05:23:22.717Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-08-28T05:23:22.718Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-28T05:23:22.811Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-28T05:23:22.820Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-28T05:23:22.821Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-28T05:23:22.821Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  05:29:37 up  3:18,  0 users,  load average: 0.16, 0.25, 0.23
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [5bde31daf45a] <==
* I0828 05:24:18.502949       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0828 05:24:18.502991       1 genericapiserver.go:752] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0828 05:24:18.950960       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0828 05:24:18.951397       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0828 05:24:18.950953       1 secure_serving.go:210] Serving securely on [::]:8443
I0828 05:24:18.951517       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0828 05:24:18.951944       1 aggregator.go:150] waiting for initial CRD sync...
I0828 05:24:18.951977       1 controller.go:83] Starting OpenAPI AggregationController
I0828 05:24:18.952016       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0828 05:24:18.952039       1 controller.go:121] Starting legacy_token_tracking_controller
I0828 05:24:18.952055       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0828 05:24:18.952089       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0828 05:24:18.952150       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0828 05:24:18.952181       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0828 05:24:18.952279       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0828 05:24:18.952323       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0828 05:24:18.952456       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0828 05:24:18.952666       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0828 05:24:18.952751       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0828 05:24:18.958828       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0828 05:24:18.961328       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0828 05:24:18.952039       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0828 05:24:18.962991       1 available_controller.go:423] Starting AvailableConditionController
I0828 05:24:18.963017       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0828 05:24:18.963121       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0828 05:24:18.963126       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0828 05:24:18.963142       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0828 05:24:18.964813       1 naming_controller.go:291] Starting NamingConditionController
I0828 05:24:18.964813       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0828 05:24:18.964890       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0828 05:24:18.965070       1 establishing_controller.go:76] Starting EstablishingController
I0828 05:24:18.965290       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0828 05:24:18.965421       1 controller.go:85] Starting OpenAPI controller
I0828 05:24:18.965476       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0828 05:24:18.965662       1 crd_finalizer.go:266] Starting CRDFinalizer
I0828 05:24:18.965984       1 controller.go:85] Starting OpenAPI V3 controller
I0828 05:24:19.110823       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0828 05:24:19.125217       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
E0828 05:24:19.131491       1 controller.go:155] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0828 05:24:19.207081       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0828 05:24:19.207116       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0828 05:24:19.207122       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0828 05:24:19.207187       1 aggregator.go:152] initial CRD sync complete...
I0828 05:24:19.207305       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0828 05:24:19.207455       1 autoregister_controller.go:141] Starting autoregister controller
I0828 05:24:19.207471       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0828 05:24:19.207479       1 cache.go:39] Caches are synced for autoregister controller
I0828 05:24:19.207532       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0828 05:24:19.207958       1 shared_informer.go:318] Caches are synced for node_authorizer
I0828 05:24:19.208107       1 shared_informer.go:318] Caches are synced for configmaps
I0828 05:24:19.756673       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0828 05:24:19.957015       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0828 05:24:21.607971       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0828 05:24:21.708014       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0828 05:24:21.838435       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0828 05:24:21.935501       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0828 05:24:21.949282       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0828 05:24:31.941005       1 controller.go:624] quota admission added evaluator for: endpoints
I0828 05:24:32.043187       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0828 05:24:32.043188       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io

* 
* ==> kube-apiserver [8bab43343013] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.588191       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.604811       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.649894       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.659545       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.671452       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.687600       1 logging.go:59] [core] [Channel #4 SubChannel #5] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0828 05:23:32.704247       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [707a597f2ede] <==
* I0828 05:19:49.774298       1 controllermanager.go:638] "Started controller" controller="pv-protection"
I0828 05:19:49.774571       1 pv_protection_controller.go:78] "Starting PV protection controller"
I0828 05:19:49.774602       1 shared_informer.go:311] Waiting for caches to sync for PV protection
I0828 05:19:49.776572       1 controllermanager.go:638] "Started controller" controller="podgc"
I0828 05:19:49.777677       1 gc_controller.go:103] Starting GC controller
I0828 05:19:49.777709       1 shared_informer.go:311] Waiting for caches to sync for GC
I0828 05:19:49.779750       1 controllermanager.go:638] "Started controller" controller="job"
I0828 05:19:49.780459       1 job_controller.go:202] Starting job controller
I0828 05:19:49.780487       1 shared_informer.go:311] Waiting for caches to sync for job
I0828 05:19:49.782630       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0828 05:19:49.797346       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0828 05:19:49.808603       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0828 05:19:49.810754       1 shared_informer.go:318] Caches are synced for daemon sets
I0828 05:19:49.831094       1 shared_informer.go:318] Caches are synced for namespace
I0828 05:19:49.832370       1 shared_informer.go:318] Caches are synced for TTL after finished
I0828 05:19:49.832875       1 shared_informer.go:318] Caches are synced for service account
I0828 05:19:49.836371       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0828 05:19:49.837639       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0828 05:19:49.842599       1 shared_informer.go:318] Caches are synced for attach detach
I0828 05:19:49.843839       1 shared_informer.go:318] Caches are synced for taint
I0828 05:19:49.843885       1 shared_informer.go:318] Caches are synced for HPA
I0828 05:19:49.844091       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0828 05:19:49.844161       1 taint_manager.go:211] "Sending events to api server"
I0828 05:19:49.844370       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0828 05:19:49.844861       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0828 05:19:49.844953       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0828 05:19:49.845290       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0828 05:19:49.845758       1 shared_informer.go:318] Caches are synced for endpoint
I0828 05:19:49.849403       1 shared_informer.go:318] Caches are synced for deployment
I0828 05:19:49.850795       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0828 05:19:49.853526       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0828 05:19:49.853915       1 shared_informer.go:318] Caches are synced for persistent volume
I0828 05:19:49.854485       1 shared_informer.go:318] Caches are synced for node
I0828 05:19:49.854544       1 range_allocator.go:174] "Sending events to api server"
I0828 05:19:49.854557       1 range_allocator.go:178] "Starting range CIDR allocator"
I0828 05:19:49.854559       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0828 05:19:49.854562       1 shared_informer.go:318] Caches are synced for cidrallocator
I0828 05:19:49.856276       1 shared_informer.go:318] Caches are synced for stateful set
I0828 05:19:49.858474       1 shared_informer.go:318] Caches are synced for cronjob
I0828 05:19:49.858600       1 shared_informer.go:318] Caches are synced for disruption
I0828 05:19:49.858662       1 shared_informer.go:318] Caches are synced for PVC protection
I0828 05:19:49.861763       1 shared_informer.go:318] Caches are synced for ReplicationController
I0828 05:19:49.862946       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0828 05:19:49.863880       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0828 05:19:49.864151       1 shared_informer.go:318] Caches are synced for expand
I0828 05:19:49.864447       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0828 05:19:49.865588       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0828 05:19:49.866808       1 shared_informer.go:318] Caches are synced for crt configmap
I0828 05:19:49.870204       1 shared_informer.go:318] Caches are synced for ephemeral
I0828 05:19:49.872566       1 shared_informer.go:318] Caches are synced for TTL
I0828 05:19:49.872668       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0828 05:19:49.875022       1 shared_informer.go:318] Caches are synced for PV protection
I0828 05:19:49.878308       1 shared_informer.go:318] Caches are synced for GC
I0828 05:19:49.880606       1 shared_informer.go:318] Caches are synced for job
I0828 05:19:49.955862       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0828 05:19:50.031669       1 shared_informer.go:318] Caches are synced for resource quota
I0828 05:19:50.083028       1 shared_informer.go:318] Caches are synced for resource quota
I0828 05:19:50.409133       1 shared_informer.go:318] Caches are synced for garbage collector
I0828 05:19:50.468608       1 shared_informer.go:318] Caches are synced for garbage collector
I0828 05:19:50.468662       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"

* 
* ==> kube-controller-manager [8405c9baf181] <==
* I0828 05:24:31.795230       1 resource_quota_monitor.go:223] "QuotaMonitor created object count evaluator" resource="poddisruptionbudgets.policy"
I0828 05:24:31.795281       1 resource_quota_monitor.go:223] "QuotaMonitor created object count evaluator" resource="roles.rbac.authorization.k8s.io"
I0828 05:24:31.795423       1 controllermanager.go:638] "Started controller" controller="resourcequota"
I0828 05:24:31.795657       1 resource_quota_controller.go:295] "Starting resource quota controller"
I0828 05:24:31.795743       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0828 05:24:31.795768       1 resource_quota_monitor.go:304] "QuotaMonitor running"
I0828 05:24:31.836243       1 controllermanager.go:638] "Started controller" controller="daemonset"
I0828 05:24:31.836436       1 daemon_controller.go:291] "Starting daemon sets controller"
I0828 05:24:31.836468       1 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0828 05:24:31.840129       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0828 05:24:31.848231       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0828 05:24:31.850612       1 shared_informer.go:318] Caches are synced for PVC protection
I0828 05:24:31.851979       1 shared_informer.go:318] Caches are synced for node
I0828 05:24:31.852066       1 range_allocator.go:174] "Sending events to api server"
I0828 05:24:31.852098       1 range_allocator.go:178] "Starting range CIDR allocator"
I0828 05:24:31.852106       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0828 05:24:31.852113       1 shared_informer.go:318] Caches are synced for cidrallocator
I0828 05:24:31.853603       1 shared_informer.go:318] Caches are synced for PV protection
I0828 05:24:31.854442       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0828 05:24:31.854751       1 shared_informer.go:318] Caches are synced for expand
I0828 05:24:31.858458       1 shared_informer.go:318] Caches are synced for ephemeral
I0828 05:24:31.862792       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0828 05:24:31.865195       1 shared_informer.go:318] Caches are synced for job
I0828 05:24:31.871438       1 shared_informer.go:318] Caches are synced for TTL after finished
I0828 05:24:31.874840       1 shared_informer.go:318] Caches are synced for endpoint
I0828 05:24:31.889156       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0828 05:24:31.891098       1 shared_informer.go:318] Caches are synced for GC
I0828 05:24:31.894413       1 shared_informer.go:318] Caches are synced for service account
I0828 05:24:31.903798       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0828 05:24:31.903933       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0828 05:24:31.904983       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0828 05:24:31.906206       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0828 05:24:31.909718       1 shared_informer.go:318] Caches are synced for taint
I0828 05:24:31.909996       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0828 05:24:31.910050       1 taint_manager.go:211] "Sending events to api server"
I0828 05:24:31.910121       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0828 05:24:31.910316       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0828 05:24:31.910540       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0828 05:24:31.910680       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0828 05:24:31.913232       1 shared_informer.go:318] Caches are synced for crt configmap
I0828 05:24:31.913317       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0828 05:24:31.914685       1 shared_informer.go:318] Caches are synced for ReplicationController
I0828 05:24:31.919018       1 shared_informer.go:318] Caches are synced for persistent volume
I0828 05:24:31.920282       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0828 05:24:31.921142       1 shared_informer.go:318] Caches are synced for cronjob
I0828 05:24:31.922606       1 shared_informer.go:318] Caches are synced for stateful set
I0828 05:24:31.926986       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0828 05:24:31.937569       1 shared_informer.go:318] Caches are synced for daemon sets
I0828 05:24:31.938549       1 shared_informer.go:318] Caches are synced for namespace
I0828 05:24:31.944227       1 shared_informer.go:318] Caches are synced for TTL
I0828 05:24:31.947697       1 shared_informer.go:318] Caches are synced for attach detach
I0828 05:24:32.017138       1 shared_informer.go:318] Caches are synced for disruption
I0828 05:24:32.040794       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0828 05:24:32.076627       1 shared_informer.go:318] Caches are synced for deployment
I0828 05:24:32.095886       1 shared_informer.go:318] Caches are synced for resource quota
I0828 05:24:32.109035       1 shared_informer.go:318] Caches are synced for HPA
I0828 05:24:32.140543       1 shared_informer.go:318] Caches are synced for resource quota
I0828 05:24:32.454631       1 shared_informer.go:318] Caches are synced for garbage collector
I0828 05:24:32.485671       1 shared_informer.go:318] Caches are synced for garbage collector
I0828 05:24:32.485719       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"

* 
* ==> kube-proxy [05389b716437] <==
* I0828 05:24:22.411845       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0828 05:24:22.412320       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0828 05:24:22.412626       1 server_others.go:554] "Using iptables proxy"
I0828 05:24:22.611085       1 server_others.go:192] "Using iptables Proxier"
I0828 05:24:22.611161       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0828 05:24:22.611170       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0828 05:24:22.611219       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0828 05:24:22.611518       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0828 05:24:22.616444       1 server.go:658] "Version info" version="v1.27.4"
I0828 05:24:22.616485       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0828 05:24:22.622452       1 config.go:188] "Starting service config controller"
I0828 05:24:22.622799       1 shared_informer.go:311] Waiting for caches to sync for service config
I0828 05:24:22.622872       1 config.go:97] "Starting endpoint slice config controller"
I0828 05:24:22.622879       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0828 05:24:22.628397       1 config.go:315] "Starting node config controller"
I0828 05:24:22.628431       1 shared_informer.go:311] Waiting for caches to sync for node config
I0828 05:24:22.724010       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0828 05:24:22.724010       1 shared_informer.go:318] Caches are synced for service config
I0828 05:24:22.729311       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [1cc7d2eb70cf] <==
* E0828 05:19:34.018998       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I0828 05:19:37.528247       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0828 05:19:37.529359       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0828 05:19:37.529617       1 server_others.go:554] "Using iptables proxy"
I0828 05:19:38.008855       1 server_others.go:192] "Using iptables Proxier"
I0828 05:19:38.009195       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0828 05:19:38.009208       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0828 05:19:38.009328       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0828 05:19:38.011322       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0828 05:19:38.018380       1 server.go:658] "Version info" version="v1.27.4"
I0828 05:19:38.018436       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0828 05:19:38.121776       1 config.go:188] "Starting service config controller"
I0828 05:19:38.122036       1 config.go:97] "Starting endpoint slice config controller"
I0828 05:19:38.122828       1 config.go:315] "Starting node config controller"
I0828 05:19:38.122864       1 shared_informer.go:311] Waiting for caches to sync for node config
I0828 05:19:38.123528       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0828 05:19:38.123774       1 shared_informer.go:311] Waiting for caches to sync for service config
I0828 05:19:38.223362       1 shared_informer.go:318] Caches are synced for node config
I0828 05:19:38.224606       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0828 05:19:38.224608       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [42fefb462bc7] <==
* I0828 05:19:34.143583       1 serving.go:348] Generated self-signed cert in-memory
W0828 05:19:37.328357       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0828 05:19:37.329069       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0828 05:19:37.329115       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0828 05:19:37.329124       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0828 05:19:37.534248       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0828 05:19:37.534305       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0828 05:19:37.609826       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0828 05:19:37.610349       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0828 05:19:37.610954       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0828 05:19:37.611552       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0828 05:19:37.711004       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0828 05:23:22.616939       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0828 05:23:22.618045       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0828 05:23:22.618299       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0828 05:23:22.619301       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E0828 05:23:22.619332       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [e06c58517694] <==
* I0828 05:24:16.555719       1 serving.go:348] Generated self-signed cert in-memory
W0828 05:24:19.030421       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0828 05:24:19.030568       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0828 05:24:19.030582       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0828 05:24:19.030590       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0828 05:24:19.133507       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0828 05:24:19.133558       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0828 05:24:19.136422       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0828 05:24:19.136903       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0828 05:24:19.137001       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0828 05:24:19.137028       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0828 05:24:19.237892       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Aug 28 05:24:21 minikube kubelet[1616]: I0828 05:24:21.312060    1616 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="65bc31b6d77dbcb19a687e1e2b869162c86d4304cd91d2c7e821d9588aa1e7d5"
Aug 28 05:24:21 minikube kubelet[1616]: I0828 05:24:21.611157    1616 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="34dae6cbe915f19b2354e6643216597b9b649fe686e60e038f6862bd8bc58a31"
Aug 28 05:24:21 minikube kubelet[1616]: I0828 05:24:21.623618    1616 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0650dd9818f4f1b945e2d7e9730bc7418d9e9a78a86eb653e6ba41e14671d150"
Aug 28 05:24:24 minikube kubelet[1616]: I0828 05:24:24.117354    1616 prober_manager.go:287] "Failed to trigger a manual run" probe="Readiness"
Aug 28 05:24:25 minikube kubelet[1616]: E0828 05:24:25.471521    1616 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 28 05:24:25 minikube kubelet[1616]: E0828 05:24:25.472951    1616 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 28 05:24:25 minikube kubelet[1616]: I0828 05:24:25.677234    1616 prober_manager.go:287] "Failed to trigger a manual run" probe="Readiness"
Aug 28 05:24:26 minikube kubelet[1616]: E0828 05:24:26.277246    1616 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:24:26 minikube kubelet[1616]: E0828 05:24:26.277354    1616 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:24:26 minikube kubelet[1616]: E0828 05:24:26.277573    1616 kuberuntime_manager.go:1212] container &Container{Name:rapid-container,Image:rapid-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rpwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod rapid-app-5c4b978988-djzm9_default(2da0a646-b7bd-4ff0-9577-85ce219a4d3b): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 28 05:24:26 minikube kubelet[1616]: E0828 05:24:26.277635    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:24:27 minikube kubelet[1616]: E0828 05:24:27.139058    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:24:32 minikube kubelet[1616]: I0828 05:24:32.177271    1616 scope.go:115] "RemoveContainer" containerID="a0a0a3d955d3a6ff0d98dd6e74ead408dc82213bd9870cdcfb3fc28452ae8f25"
Aug 28 05:24:32 minikube kubelet[1616]: I0828 05:24:32.177809    1616 scope.go:115] "RemoveContainer" containerID="1240730520b5d61f62e0d117e4bedf677fdeeeb03cae76758df8354234bfef28"
Aug 28 05:24:32 minikube kubelet[1616]: E0828 05:24:32.177994    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c0d5b174-8c19-4600-8bbd-b330374c9478)\"" pod="kube-system/storage-provisioner" podUID=c0d5b174-8c19-4600-8bbd-b330374c9478
Aug 28 05:24:36 minikube kubelet[1616]: E0828 05:24:36.522390    1616 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 28 05:24:36 minikube kubelet[1616]: E0828 05:24:36.522433    1616 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 28 05:24:44 minikube kubelet[1616]: E0828 05:24:44.511370    1616 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:24:44 minikube kubelet[1616]: E0828 05:24:44.511414    1616 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:24:44 minikube kubelet[1616]: E0828 05:24:44.511556    1616 kuberuntime_manager.go:1212] container &Container{Name:rapid-container,Image:rapid-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rpwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod rapid-app-5c4b978988-djzm9_default(2da0a646-b7bd-4ff0-9577-85ce219a4d3b): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 28 05:24:44 minikube kubelet[1616]: E0828 05:24:44.511611    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:24:44 minikube kubelet[1616]: I0828 05:24:44.630873    1616 scope.go:115] "RemoveContainer" containerID="1240730520b5d61f62e0d117e4bedf677fdeeeb03cae76758df8354234bfef28"
Aug 28 05:24:47 minikube kubelet[1616]: E0828 05:24:47.573392    1616 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 28 05:24:47 minikube kubelet[1616]: E0828 05:24:47.573445    1616 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 28 05:24:55 minikube kubelet[1616]: E0828 05:24:55.634050    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:24:58 minikube kubelet[1616]: E0828 05:24:58.628091    1616 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 28 05:24:58 minikube kubelet[1616]: E0828 05:24:58.628140    1616 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 28 05:25:09 minikube kubelet[1616]: E0828 05:25:09.680158    1616 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 28 05:25:09 minikube kubelet[1616]: E0828 05:25:09.680211    1616 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 28 05:25:11 minikube kubelet[1616]: E0828 05:25:11.278474    1616 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:25:11 minikube kubelet[1616]: E0828 05:25:11.278531    1616 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:25:11 minikube kubelet[1616]: E0828 05:25:11.278634    1616 kuberuntime_manager.go:1212] container &Container{Name:rapid-container,Image:rapid-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rpwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod rapid-app-5c4b978988-djzm9_default(2da0a646-b7bd-4ff0-9577-85ce219a4d3b): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 28 05:25:11 minikube kubelet[1616]: E0828 05:25:11.278670    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:25:23 minikube kubelet[1616]: E0828 05:25:23.632036    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:25:35 minikube kubelet[1616]: E0828 05:25:35.631868    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:25:47 minikube kubelet[1616]: E0828 05:25:47.640267    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:26:03 minikube kubelet[1616]: E0828 05:26:03.391382    1616 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:26:03 minikube kubelet[1616]: E0828 05:26:03.391473    1616 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:26:03 minikube kubelet[1616]: E0828 05:26:03.391611    1616 kuberuntime_manager.go:1212] container &Container{Name:rapid-container,Image:rapid-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rpwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod rapid-app-5c4b978988-djzm9_default(2da0a646-b7bd-4ff0-9577-85ce219a4d3b): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 28 05:26:03 minikube kubelet[1616]: E0828 05:26:03.391657    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:26:14 minikube kubelet[1616]: E0828 05:26:14.629027    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:26:28 minikube kubelet[1616]: E0828 05:26:28.638287    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:26:39 minikube kubelet[1616]: E0828 05:26:39.628820    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:26:53 minikube kubelet[1616]: E0828 05:26:53.633904    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:27:05 minikube kubelet[1616]: E0828 05:27:05.634686    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:27:19 minikube kubelet[1616]: E0828 05:27:19.629737    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:27:37 minikube kubelet[1616]: E0828 05:27:37.371922    1616 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:27:37 minikube kubelet[1616]: E0828 05:27:37.372102    1616 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rapid-app:latest"
Aug 28 05:27:37 minikube kubelet[1616]: E0828 05:27:37.372397    1616 kuberuntime_manager.go:1212] container &Container{Name:rapid-container,Image:rapid-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rpwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod rapid-app-5c4b978988-djzm9_default(2da0a646-b7bd-4ff0-9577-85ce219a4d3b): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 28 05:27:37 minikube kubelet[1616]: E0828 05:27:37.372480    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for rapid-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:27:50 minikube kubelet[1616]: E0828 05:27:50.632580    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:28:04 minikube kubelet[1616]: E0828 05:28:04.643134    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:28:18 minikube kubelet[1616]: E0828 05:28:18.632343    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:28:32 minikube kubelet[1616]: E0828 05:28:32.632812    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:28:43 minikube kubelet[1616]: E0828 05:28:43.636490    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:28:55 minikube kubelet[1616]: E0828 05:28:55.636367    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:29:07 minikube kubelet[1616]: E0828 05:29:07.635902    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:29:14 minikube kubelet[1616]: W0828 05:29:14.235134    1616 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 28 05:29:19 minikube kubelet[1616]: E0828 05:29:19.637867    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b
Aug 28 05:29:33 minikube kubelet[1616]: E0828 05:29:33.638020    1616 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rapid-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rapid-app:latest\\\"\"" pod="default/rapid-app-5c4b978988-djzm9" podUID=2da0a646-b7bd-4ff0-9577-85ce219a4d3b

* 
* ==> storage-provisioner [1240730520b5] <==
* I0828 05:24:22.017611       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0828 05:24:32.034281       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

* 
* ==> storage-provisioner [198eeb19dcdf] <==
* I0828 05:24:44.785487       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0828 05:24:44.798574       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0828 05:24:44.799104       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0828 05:25:02.211164       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0828 05:25:02.211462       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8377d2d0-24fc-448c-87e1-96f949571011!
I0828 05:25:02.211539       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"803a0fcd-016b-4693-86ed-59112ae4e0ac", APIVersion:"v1", ResourceVersion:"5130", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8377d2d0-24fc-448c-87e1-96f949571011 became leader
I0828 05:25:02.312907       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8377d2d0-24fc-448c-87e1-96f949571011!

